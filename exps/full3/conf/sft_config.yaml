# 模型设置
model: "/dfs/data/models/Qwen3-4B-Instruct-2507"
loss_scale: "default"  # 全部回复计算loss
model_type: "qwen3"
template: "qwen3_nothinking"
train_type: "full"

# 数据设置
dataset:
  - "/dfs/data/work/hardtry/data/gem/gem_openai_messages_fc.json"
max_length: 10240
split_dataset_ratio: 0.05
# overlong_filter: true

# 显存优化
attn_impl: "flash_attention_2"
gradient_checkpointing: true
packing: true

# 硬件相关
torch_dtype: "bfloat16"
dataloader_num_workers: 8

# 训练参数
# per_device_train_batch_size 与 gradient_accumulation_steps 强关联
# 在4卡情况下
#   per_device_train_batch_size * gradient_accumulation_steps == 16
# 即可
num_train_epochs: 1
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
learning_rate: 1e-4
warmup_ratio: 0.05
lr_scheduler_type: "cosine"
gradient_accumulation_steps: 16
max_grad_norm: 1.0
weight_decay: 0.01

# LoRA
# lora_rank: 16
# lora_alpha: 32
# lora_dropout: 0.01
# target_modules: "all-linear"

# 监控与保存
eval_steps: 10
save_steps: 10
save_total_limit: 2
logging_steps: 10
eval_strategy: "steps"
save_strategy: "steps"
eval_on_start: true
output_dir: "/dfs/data/work/hardtry/checkpoints/full3"
seed: 42

# SwanLab
report_to: "swanlab"
swanlab_project: "hardtry"
swanlab_exp_name: "full3"

fsdp: "fsdp2"

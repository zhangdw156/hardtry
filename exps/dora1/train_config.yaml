# ModelArguments
model_name_or_path: "/dfs/data/models/Qwen3-4B-Thinking-2507"
use_4bit_quant: true
attn_implementation: "sdpa"

# ScriptArguments (自定义参数)
data_path: "../../data/bfcl_multi_turn.json"
train_subset_size: 5000
validation_split_percentage: 0.05
total_batch_size: 64

# LoRA / DoRA
lora_r: 16
lora_alpha: 32
use_dora: true
target_modules:
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"
system_prompt_save_path: "system_prompt_check.txt"
prompt_demo_save_path: "promt_demo_check.txt"

# TrainingArguments (HF 标准参数)
output_dir: "../../checkpoints/dora1"

# 1. 显存优化关键参数 (默认是 False，必须手动开启)
gradient_checkpointing: true
group_by_length: true

# 2. 优化器设置
optim: "paged_adamw_32bit"    

# 3. 分布式相关
ddp_find_unused_parameters: false

# 4. 基础参数
per_device_train_batch_size: 1
num_train_epochs: 1
learning_rate: 0.0001
bf16: true
fp16: false  # (虽然默认是 false，显式写出来更清晰)

# 5. 验证与保存
eval_strategy: "steps"
eval_steps: 10
logging_steps: 10
save_strategy: "epoch"
report_to: "swanlab"
deepspeed: "ds_config.json"
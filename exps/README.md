## 记录所有实验信息

所有的结果都是在bfclv4的mutil turn base上进行5次实验

目前看来，全量rl有效果且效果不错

### lora1(似乎没有效果)

> ms-swift

> SFT, 使用所有回复计算loss

实验结果

- 26.50%
- 27.00%
- 27.00%
- 26.50%
- 25.50%

### lora2(似乎没有效果)

> ms-swift

> SFT, 使用最后一轮回复计算loss

实验结果

- 26.50%
- 24.50%
- 26.50%
- 27.00%
- 25.50%

### verl1(有效)

> verl

> GRPO强化学习

实验结果

- 45.00%
- 45.50%
- 45.50%
- 45.50%
- 47.00%

### verl2(几乎没有提升)

> verl

> 带lora的GRPO强化学习

实验结果

- 26.00%
- 27.50%
- 26.50%
- 26.50%
- 26.50%

### full1(无效，甚至降低)

> ms-swift

> 全量SFT，所有回复计算loss

实验结果

- 24.50%
- 23.50%
- 23.00%
- 24.00%
- 22.50%

### full2(效果最差)

> ms-swift

> 全量SFT，最后一轮回复计算loss

实验结果

- 10.50%
- 11.00%
- 10.50%
- 10.50%
- 11.00%

### baseline

> 用于对照

实验结果

- 26.5%
- 26.5%
- 26.5%
- 27.5%
- 27.5%
使用transformer+peft进行4-bit量化和bf16的QDoRA微调

累计batch size为64

使用deepspeed的zero2

跑了1个epoch

学习率为1e-4

使用了hardgen的5k条数据
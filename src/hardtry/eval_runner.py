import os
import sys
import shutil
import time
import subprocess
import logging
from concurrent.futures import ProcessPoolExecutor
from datetime import datetime
from dataclasses import dataclass, field
from typing import Optional
from transformers import HfArgumentParser

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class EvalArguments:
    # --- æ ¸å¿ƒä»»åŠ¡é…ç½® ---
    model_name: str = field(metadata={"help": "Model name for bfcl (e.g., Qwen/Qwen3-4B-FC)"})
    test_category: str = field(default="multi_turn_base", metadata={"help": "Test category for bfcl"})
    
    # --- è¿è¡Œç¯å¢ƒé…ç½® ---
    venv_activate_path: str = field(default="/dfs/data/uv-venv/gorilla/bin/activate", metadata={"help": "Path to venv activate script"})
    remote_openai_tokenizer_path: str = field(default="", metadata={"help": "Path to local tokenizer for vllm backend"})
    remote_openai_base_url: str = field(default="http://localhost:8000/v1")
    remote_openai_api_key: str = field(default="EMPTY")
    
    # --- å¹¶è¡Œæ§åˆ¶ ---
    num_runs: int = field(default=5, metadata={"help": "Total number of parallel runs"})
    threads_per_run: int = field(default=32, metadata={"help": "Number of threads for each bfcl process"})
    
    # --- è·¯å¾„æ§åˆ¶ ---
    base_artifact_dir: str = field(
        default="./eval_results", 
        metadata={"help": "Root directory to store all run logs and results"}
    )
    experiment_name: str = field(
        default="default_exp", 
        metadata={"help": "Name of the experiment to create subfolder"}
    )
    # æŒ‡å®šæ”¶é›†ç»“æœçš„ç›®æ ‡æ–‡ä»¶å¤¹
    summary_output_dir: Optional[str] = field(
        default=None, 
        metadata={"help": "Directory to copy the collected CSV results. If None, defaults to <output_dir>/summary_csvs"}
    )

class ParallelEvalRunner:
    def __init__(self, args: EvalArguments):
        self.args = args
        
        # æ„é€ å®éªŒçš„æ ¹è¾“å‡ºç›®å½•: base_artifact_dir/experiment_name
        self.output_dir = os.path.join(self.args.base_artifact_dir, self.args.experiment_name)
        
        # æ„é€ ç¯å¢ƒå˜é‡
        self.env_vars = {
            "REMOTE_OPENAI_BASE_URL": self.args.remote_openai_base_url,
            "REMOTE_OPENAI_API_KEY": self.args.remote_openai_api_key,
            "REMOTE_OPENAI_TOKENIZER_PATH": self.args.remote_openai_tokenizer_path,
            "PATH": os.environ.get("PATH", "")
        }

    def run_single_eval(self, run_id):
        """å•ä¸ªå®éªŒä»»åŠ¡"""
        timestamp = datetime.now().strftime("%m%d_%H%M%S")
        
        # å…·ä½“çš„æŸä¸€æ¬¡è¿è¡Œç›®å½•
        run_dir = os.path.join(self.output_dir, f"run_{run_id}_{timestamp}")
        os.makedirs(run_dir, exist_ok=True)
        
        log_file_path = os.path.join(run_dir, "experiment.log")
        
        # æ„é€ å‘½ä»¤
        cmd = f"""
        source {self.args.venv_activate_path}
        echo "--- Start Generation [Run {run_id}] ---"
        bfcl generate --model "{self.args.model_name}" --test-category "{self.args.test_category}" --backend vllm --skip-server-setup --num-threads "{self.args.threads_per_run}" --result-dir "{run_dir}/result"
        echo "--- Start Evaluation [Run {run_id}] ---"
        bfcl evaluate --model "{self.args.model_name}" --test-category "{self.args.test_category}" --result-dir "{run_dir}/result" --score-dir "{run_dir}/score"
        """

        logger.info(f"ğŸš€ [Run {run_id}] Started. Log: {log_file_path}")
        
        with open(log_file_path, "w") as log_file:
            try:
                result = subprocess.run(
                    ["bash", "-c", cmd],
                    env={**os.environ, **self.env_vars},
                    stdout=log_file,
                    stderr=subprocess.STDOUT,
                    text=True
                )
                if result.returncode == 0:
                    logger.info(f"âœ… [Run {run_id}] Completed successfully.")
                    return True, run_id, run_dir
                else:
                    logger.error(f"âŒ [Run {run_id}] Failed. Check log.")
                    return False, run_id, run_dir
            except Exception as e:
                logger.error(f"ğŸ’¥ [Run {run_id}] Exception: {e}")
                return False, run_id, run_dir

    def collect_results(self):
        """æ”¶é›†å¹¶é‡å‘½å csv ç»“æœæ–‡ä»¶"""
        
        # åˆ¤æ–­ç”¨æˆ·æ˜¯å¦æŒ‡å®šäº†æ±‡æ€»è·¯å¾„
        if self.args.summary_output_dir:
            # å¦‚æœç”¨æˆ·æŒ‡å®šäº†è·¯å¾„ï¼Œå°±ç”¨ç”¨æˆ·çš„
            dest_dir = self.args.summary_output_dir
        else:
            # å¦‚æœæ²¡æŒ‡å®šï¼Œé»˜è®¤æ”¾åœ¨å®éªŒç›®å½•ä¸‹çš„ summary_csvs
            dest_dir = os.path.join(self.output_dir, "summary_csvs")
            
        os.makedirs(dest_dir, exist_ok=True)
        
        logger.info(f"\nğŸ“¦ [Collection] Collecting results to: {dest_dir}")

        if not os.path.exists(self.output_dir):
            logger.warning("âŒ Artifact directory does not exist.")
            return

        count = 0
        for folder_name in sorted(os.listdir(self.output_dir)):
            run_path = os.path.join(self.output_dir, folder_name)
            
            # ç¡®ä¿æ˜¯æ–‡ä»¶å¤¹ä¸”ä»¥ run_ å¼€å¤´
            if os.path.isdir(run_path) and folder_name.startswith("run_"):
                # åŸå§‹æ–‡ä»¶è·¯å¾„
                src_file = os.path.join(run_path, "score", "data_multi_turn.csv")
                
                if os.path.exists(src_file):
                    new_filename = f"data_multi_turn_{folder_name}.csv"
                    dest_file = os.path.join(dest_dir, new_filename)
                    
                    try:
                        shutil.copy(src_file, dest_file)
                        logger.info(f"  -> Copied: {new_filename}")
                        count += 1
                    except Exception as e:
                        logger.error(f"  âŒ Copy failed {folder_name}: {e}")
                else:
                    logger.warning(f"  âš ï¸ File not found: {src_file}")

        logger.info(f"âœ… Collection complete. Copied {count} files.\n")

    def run(self):
        print("=======================================================")
        print(f"ğŸ”¥ Parallel Evaluation ({self.args.num_runs} runs)")
        print(f"ğŸ¤– Model: {self.args.model_name}")
        print(f"ğŸ“‚ Output Dir: {self.output_dir}")
        print("=======================================================\n")

        start_time = time.time()

        # ä½¿ç”¨è¿›ç¨‹æ± 
        with ProcessPoolExecutor(max_workers=self.args.num_runs) as executor:
            futures = executor.map(self.run_single_eval, range(1, self.args.num_runs + 1))
            results = list(futures)

        end_time = time.time()
        
        success_count = sum(1 for r in results if r[0])
        
        print(f"\n" + "="*55)
        print(f"ğŸ All tasks finished!")
        print(f"Success: {success_count}/{self.args.num_runs}")
        print(f"Total Time: {(end_time - start_time)/60:.2f} mins")
        print("="*55)
        
        self.collect_results()

if __name__ == "__main__":
    parser = HfArgumentParser((EvalArguments,))
    
    if len(sys.argv) == 2 and sys.argv[1].endswith(".yaml"):
        eval_args, = parser.parse_yaml_file(yaml_file=sys.argv[1])
    else:
        eval_args, = parser.parse_args_into_dataclasses()

    runner = ParallelEvalRunner(eval_args)
    runner.run()